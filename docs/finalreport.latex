% This is a template to create your final report as a pdf.
% Just fill in the areas and run pdflatex FinalReport.tex
% For more information on LaTeX documents consult The (Not So
% Short Introduction to LateX2e

\documentclass[12pt]{article}
\usepackage[pdftex]{graphicx,color}
\usepackage{amsmath}
\def\baselinestretch{1.5} % line spaceing
\title{ECE437L Final Report \\ TA: Eric Villasenor}
\author{Nabeel Zaim \and Steven Ellis} 
\date{May 2, 2014}

\begin{document}
  \maketitle
  
  \pagebreak

  \section{Executive Summary}
  
  We have designed several implementations of a MIPS CPU. Initially, a single-cycle CPU was designed that executes (roughly) a single instruction each clock cycle. That was built into a 5-stage pipelined CPU which allows the overlapping of several instructions in order to increase the clock rate. Afterwards, caches were added to reduce the average memory access time. Finally, the design was converted into a dual-core CPU, implementing a bus coherence controller connecting the cores. All of these designs have been successful.
  
  During the process of creating these processors, many insights into processor design were gained. One of the most imporant of these is the importance of modularity, which is tremendously helpful when incrementally adding new features to a design by enabling large amounts of code reuse, assisted fully by SystemVerilog's parameterization feature. This was incredibly helpful in creating multiple pipeline latches, all of which were internally identical but had different bus widths. Another one was that making things into seperate Verilog modules was necessary but not sufficient for useful modularity. If a module made too many assumptions about the other pieces it is connected to (such as the latency of memory), then it becomes too specific to be easily reusable, largely defeating the point of the modularity. The other key importance of modularity besides code reuse was as an abstraction tool, helping to break a highly complex system into smaller, more managable pieces.
  
  In the rest of the report, the design and verification process of both the caches and the cache coherence implementation will be discussed. There will also be a listing and analysis of the performance of the different designs.
  \pagebreak
  
  \section{Processor Design and Verification}
  
  \subsection{Cache Design}
  
  1KB 2-way set associative with each block containing 2 words as well as valid, dirty, and state bits.

Implemented as a state machine since filling or flushing a multiple-word block requires multiple memory cycles (the memory can only supply one word at a time).

Description of states. Fetch, Writeback, Flush.

  Within the caches, each block holds an MSI state. It arbitrates with other caches by sending BusRd, BusRdX and BusCache signals onto the bus. 

The MSI protocol was tested by running it through a parallel program (similar to dual.llsc) that would utilize the same address block. Only a single address was changed to keep it simple - the block in dcache was monitored for state transitions while one core would write and the other would read, and then vice versa. 

Verification of dcache involved testing reading and writing. Data incurring cumpulsory misses (tests fetch states), conflict misses with no dirty block, conflict misses with dirty block, working on data with index match but different tag (tests set associativity), flushing.

The common case of dcache use is one where a block is brought into cache and used in the near future. Situations that would degrade the performance would be one such as two cores constantly writing to and reading from a single address. Each cache would be constantly invalidating the other, thus saturating the bus and allowing few cache hits.

  
  \subsection{Multicore Design}
  The CPU designed utilizes a shared memory model architecture - one where multiple processors work on data in a shared address space, communicating over a single shared bus connecting the cores to each other and to main memory. Figure 1 shows the arrangement of the datapath and cache components of the CPU with respect to the memory.

  In the process of implementing multiple cores, hardware has been added to support write atomicity through \textbf{load-link/store-conditional} instructions. The address to work on is stored in a link register, which is a latch with two fields - a 32-bit address field and a 1-bit valid field. This allowed the implementation to be kept simple. The register may be written to or read from but its result would only hold value (i.e. denote a true atomic operation) when paired with a high valid bit.
  
  One key change that needed to be made to implement multicore was to replace the memory controller with a bus controller, for a bus which connects to RAM and both CPUs. This bus controller arbitrates between the two CPU cores, facilitating memory access and transitions in the MSI state machine for cache coherence. The bus controller is implemented with its own state machine, different from but tightly interconnected with the MSI state machine. One of the important design decisions in a multicore CPU is how to implement this bus controller and its state machine.
  
  The bus controller state machine starts out in an idle state, which it remains in as long as there is no activity on the bus. Whenever one or both of the CPU cores initiates a bus transaction, the bus controller transitions to a state giving one of the cores control over the bus. The arbitration gives control to whoever comes first, and in the case of both cores requesting at the same time, core 0 wins. The next state sends out signals for a snoop operation, where the requesting core will inform the other core if it needs to invalidate a block or perform a cache to cache transfer. After snooping, if the other core is forced to do a writeback by the coherence operation, a new state is entered for the writeback. If the writeback isn't needed or after the writeback completes, a memory access state is entered which allows the cache to access memory for either a read or a write. Once that core indicates that it has completed its memory transaction, the bus controller then returns to idle.

  \pagebreak
  \subsection{Diagrams}

  \begin{figure}[hp]
      \includegraphics[width=\textwidth,angle=90]{pipeline_datapath.pdf}
    \caption{Pipeline Diagram}
  \end{figure}

    \begin{figure}[hp]
      \includegraphics[width=\textwidth]{caches.pdf}
    \caption{Data Cache Diagram}
  \end{figure}
  
   \begin{figure}[hp]
      \includegraphics[width=\textwidth]{cache_coherence_state_diagram.png}
    \caption{Data Cache Diagram}
  \end{figure}
  

  \begin{figure}[hp]
      \includegraphics[width=\textwidth]{multicore.png}
    \caption{Multicore Block Diagram}
   \end{figure}
   \pagebreak

  \section{Results}
  
    \begin{table}[!hbp]

    \begin{tabular}{|l|l|l|}
      \hline
       & Singlecycle & Pipeline \\ \hline
      Logic Elements Used & 3,542 & 3,518 \\ \hline
      Registers Used & 1252 & 1672 \\ \hline
      Minimum Clock Cycle & 24.3ns & 12.2ns \\ \hline
      CPI without cache (1 cycle RAM) & 1.4 cycles & 1.8 cycles\\ \hline      
    \end{tabular}

    \caption{Synthesis results comparison using fib.asm}
  \end{table}
  
      \begin{table}[!hbp]

    \begin{tabular}{|l|l|l|}
      \hline
      & Pipeline with cache & Multicore \\ \hline
      Logic Elements & 9,662 & 23,310\\ \hline
      Registers Used & 4135 & 8221 \\ \hline
      Minimum Clock Cycle & 17.0ns & 25.2ns \\ \hline
    \end{tabular}
        \caption{Cached pipeline and multicore comparision}

      \end{table}

    
  \begin{table}[!hbp]
    \begin{tabular}{|l|l|l|l|}
      \hline
      & Pipeline without cache & Pipeline with cache & Multicore \\ \hline
      1 cycle RAM & 1.8 & 2.0 & 2.3 \\ \hline
      5 cycle RAM & 3.0* & 2.8 & 3.1 \\ \hline
    \end{tabular}
    \caption{CPI Results}
  \end{table}

  
  * = estimated result for non-working design
  
  The logic elements and registers used indicate how much of the FPGA is being used, these were taken directly from the Quartus log files. As expected, these increased with each new design's increased complexity. In particular, caches resulted in a huge number of registers because each bit of cache is one. And ulticore approximately doubled each, as would be expected.
  
  Minimum clock period gives the total delay through the system's critical path, and was found as the inverse of the max frequency (1200mV 0C Model) given in the log files. The pipeline decreased the length of the critical path, which is expected becaue that is the entire point of pipelining. Caches made the critical path longer, and multicore longer than that.

  For the CPI numbers, fib.asm was run on each of the CPUs. To find the number of instructions, it was run in the MIPS simulator which reports the instruction count when it finishes executing, which was 160 instructions. Then when simulating in hardware, the testbench prints out the number of clock cycles. Since the CPU was running off of a clock divider at half the system clock frequency, the number of CPU cycles taken was half of what the testbench reported. Then the CPI calculation was a direct application of the definition, number of cycles/number of instructions. For the singlecycle, each instruction took 1 cycle unless it was a LW/SW, which took 2. With 40 percent of dynamic instructions in fib.asm being LW/SW, the CPI would be expected to be 1.4 which it was. Then the CPI got progressively worse in each implemtation, with 1 cycle RAM latency. First the pipeline added stalls, then the cache added miss penalties, then the multicore added coherence operations, all of which increases the CPI. However, with a higher RAM latency the cache decreates average memory access time and achieves better CPI, but coherence still brings it back down somewhat.

  In the multicore CPU, the critical path is from the RAM address to the memory/writeback latch in the CPU. This implies that the critical path is through the coherence controller and the data cache, which makes a lot of sense. First of all, the fact that it was slower than the single core pipeline with cache immediately points to the coherence controller and/or data caches being the critical path, because they are the only parts that changed in the multicore design, aside from some small additions to the datapath to add the LL and SC instructions. Also, the datapath of each CPU is pipelined to reduce its critical path but the coherence bus controller is a simple single cycle implentation. Obviously without pipelining we should expect it to have a fairly slow maximum clock.
  
  The cache hierarchy influences the instruction throughput because it determines the average memory access time. Because the cache in this design is quite small and with low associativity, the hit rate, hit penalty, and miss penalty will all be low. 
  
  %TODO 
  %Multicore instruction latency. Throughput is easy but I have no idea how to figure out latency.
  %Discuss how the cache hierarchy influences instruction throughput.
  %Run your parallel algorithm on the multi-core gate level net list with timing included for the following clock periods 100ns/50ns/20ns/10ns/2ns. Discuss any observations or failures you experience.
  
\begin{verbatim}
ori $1,$0,
end
\end{verbatim}
  
  \pagebreak
  \section{Conclusion}
  %the paragraph describing the conclusion and the grading rubric do not really match each other
  
  %summary
  
  The single cycle processor worked. The pipelined implementation worked, but broke with increased memory latency. The issues causing it to fail were remedied in the version with caches attached. The cached pipelined CPU worked for a reasonably wide range of memory latency (1 to 10 clock cycles). The dual core processor with cache coherence worked equally alike. %assuming palgorithm gets fixed
  
  %lessons learned
  
  %advantages and disadvantages of processors

  The singlecycle has the advantage of a low CPI, but suffers from a low clockspeed as well. The pipeline increases the clock but drops CPI, gaining some performance but with dramatically higher power consumption. Caches bring both the CPI and clock down, while taking up substantially more area. The multicore adds the ability to run 2 programs simultaneously to potentially double the performance, but loses a large fraction of that performance gain to a combination of lower clock speed and higher CPI and of course doubles area and power consumption.

\end{document}
